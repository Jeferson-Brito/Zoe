\n--- Error at jeffe ---\nTraceback (most recent call last):
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 3047, in _generate
    response: GenerateContentResponse = self.client.models.generate_content(
                                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        **request,
        ^^^^^^^^^^
    )
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\models.py", line 5227, in generate_content
    response = self._generate_content(
        model=model, contents=contents, config=parsed_config
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\models.py", line 4009, in _generate_content
    response = self._api_client.request(
        'post', path, request_dict, http_options
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1386, in request
    response = self._request(http_request, http_options, stream=False)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1220, in _request
    return retry(self._request_once, http_request, stream)  # type: ignore[no-any-return]
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 378, in iter
    result = action(retry_state)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 400, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ~~~~~~~~~~~~~~~~~^^
  File "C:\Users\jeffe\AppData\Local\Programs\Python\Python313\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ~~~~~~~~~~~~~~~~~^^
  File "C:\Users\jeffe\AppData\Local\Programs\Python\Python313\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1199, in _request_once
    errors.APIError.raise_for_response(response)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\errors.py", line 121, in raise_for_response
    cls.raise_error(response.status_code, response_json, response)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\errors.py", line 146, in raise_error
    raise ClientError(status_code, response_json, response)
google.genai.errors.ClientError: 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\jeffe\Documents\Sites\LIA\chat\services.py", line 67, in chat
    response = self.llm.invoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=message_text)
    ])
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 2535, in invoke
    return super().invoke(input, config, stop=stop, **kwargs)
           ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 402, in invoke
    self.generate_prompt(
    ~~~~~~~~~~~~~~~~~~~~^
        [self._convert_input(input)],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<6 lines>...
        **kwargs,
        ^^^^^^^^^
    ).generations[0][0],
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1121, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 931, in generate
    self._generate_with_cache(
    ~~~~~~~~~~~~~~~~~~~~~~~~~^
        m,
        ^^
    ...<2 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1233, in _generate_with_cache
    result = self._generate(
        messages, stop=stop, run_manager=run_manager, **kwargs
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 3051, in _generate
    _handle_client_error(e, request)
    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 145, in _handle_client_error
    raise ChatGoogleGenerativeAIError(msg) from e
langchain_google_genai.chat_models.ChatGoogleGenerativeAIError: Error calling model 'gemini-1.5-flash' (NOT_FOUND): 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}
\n--------------------------------\n\n--- Error at jeffe ---\nTraceback (most recent call last):
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 3047, in _generate
    response: GenerateContentResponse = self.client.models.generate_content(
                                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        **request,
        ^^^^^^^^^^
    )
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\models.py", line 5227, in generate_content
    response = self._generate_content(
        model=model, contents=contents, config=parsed_config
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\models.py", line 4009, in _generate_content
    response = self._api_client.request(
        'post', path, request_dict, http_options
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1386, in request
    response = self._request(http_request, http_options, stream=False)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1220, in _request
    return retry(self._request_once, http_request, stream)  # type: ignore[no-any-return]
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 378, in iter
    result = action(retry_state)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 400, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ~~~~~~~~~~~~~~~~~^^
  File "C:\Users\jeffe\AppData\Local\Programs\Python\Python313\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ~~~~~~~~~~~~~~~~~^^
  File "C:\Users\jeffe\AppData\Local\Programs\Python\Python313\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1199, in _request_once
    errors.APIError.raise_for_response(response)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\errors.py", line 121, in raise_for_response
    cls.raise_error(response.status_code, response_json, response)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\errors.py", line 146, in raise_error
    raise ClientError(status_code, response_json, response)
google.genai.errors.ClientError: 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-flash-latest is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\jeffe\Documents\Sites\LIA\chat\services.py", line 67, in chat
    response = self.llm.invoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=message_text)
    ])
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 2535, in invoke
    return super().invoke(input, config, stop=stop, **kwargs)
           ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 402, in invoke
    self.generate_prompt(
    ~~~~~~~~~~~~~~~~~~~~^
        [self._convert_input(input)],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<6 lines>...
        **kwargs,
        ^^^^^^^^^
    ).generations[0][0],
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1121, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 931, in generate
    self._generate_with_cache(
    ~~~~~~~~~~~~~~~~~~~~~~~~~^
        m,
        ^^
    ...<2 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1233, in _generate_with_cache
    result = self._generate(
        messages, stop=stop, run_manager=run_manager, **kwargs
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 3051, in _generate
    _handle_client_error(e, request)
    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 145, in _handle_client_error
    raise ChatGoogleGenerativeAIError(msg) from e
langchain_google_genai.chat_models.ChatGoogleGenerativeAIError: Error calling model 'gemini-1.5-flash-latest' (NOT_FOUND): 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-flash-latest is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}
\n--------------------------------\n\n--- Error at jeffe ---\nTraceback (most recent call last):
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 3047, in _generate
    response: GenerateContentResponse = self.client.models.generate_content(
                                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        **request,
        ^^^^^^^^^^
    )
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\models.py", line 5227, in generate_content
    response = self._generate_content(
        model=model, contents=contents, config=parsed_config
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\models.py", line 4009, in _generate_content
    response = self._api_client.request(
        'post', path, request_dict, http_options
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1386, in request
    response = self._request(http_request, http_options, stream=False)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1220, in _request
    return retry(self._request_once, http_request, stream)  # type: ignore[no-any-return]
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 378, in iter
    result = action(retry_state)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 400, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ~~~~~~~~~~~~~~~~~^^
  File "C:\Users\jeffe\AppData\Local\Programs\Python\Python313\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ~~~~~~~~~~~~~~~~~^^
  File "C:\Users\jeffe\AppData\Local\Programs\Python\Python313\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1199, in _request_once
    errors.APIError.raise_for_response(response)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\errors.py", line 121, in raise_for_response
    cls.raise_error(response.status_code, response_json, response)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\errors.py", line 146, in raise_error
    raise ClientError(status_code, response_json, response)
google.genai.errors.ClientError: 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-flash-latest is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\jeffe\Documents\Sites\LIA\chat\services.py", line 67, in chat
    response = self.llm.invoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=message_text)
    ])
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 2535, in invoke
    return super().invoke(input, config, stop=stop, **kwargs)
           ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 402, in invoke
    self.generate_prompt(
    ~~~~~~~~~~~~~~~~~~~~^
        [self._convert_input(input)],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<6 lines>...
        **kwargs,
        ^^^^^^^^^
    ).generations[0][0],
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1121, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 931, in generate
    self._generate_with_cache(
    ~~~~~~~~~~~~~~~~~~~~~~~~~^
        m,
        ^^
    ...<2 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1233, in _generate_with_cache
    result = self._generate(
        messages, stop=stop, run_manager=run_manager, **kwargs
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 3051, in _generate
    _handle_client_error(e, request)
    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 145, in _handle_client_error
    raise ChatGoogleGenerativeAIError(msg) from e
langchain_google_genai.chat_models.ChatGoogleGenerativeAIError: Error calling model 'gemini-1.5-flash-latest' (NOT_FOUND): 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-flash-latest is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}
\n--------------------------------\n\n--- Error at jeffe ---\nTraceback (most recent call last):
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 3047, in _generate
    response: GenerateContentResponse = self.client.models.generate_content(
                                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        **request,
        ^^^^^^^^^^
    )
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\models.py", line 5227, in generate_content
    response = self._generate_content(
        model=model, contents=contents, config=parsed_config
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\models.py", line 4009, in _generate_content
    response = self._api_client.request(
        'post', path, request_dict, http_options
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1386, in request
    response = self._request(http_request, http_options, stream=False)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1220, in _request
    return retry(self._request_once, http_request, stream)  # type: ignore[no-any-return]
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 378, in iter
    result = action(retry_state)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 400, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ~~~~~~~~~~~~~~~~~^^
  File "C:\Users\jeffe\AppData\Local\Programs\Python\Python313\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ~~~~~~~~~~~~~~~~~^^
  File "C:\Users\jeffe\AppData\Local\Programs\Python\Python313\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1199, in _request_once
    errors.APIError.raise_for_response(response)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\errors.py", line 121, in raise_for_response
    cls.raise_error(response.status_code, response_json, response)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\errors.py", line 146, in raise_error
    raise ClientError(status_code, response_json, response)
google.genai.errors.ClientError: 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-flash-001 is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\jeffe\Documents\Sites\LIA\chat\services.py", line 67, in chat
    response = self.llm.invoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=message_text)
    ])
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 2535, in invoke
    return super().invoke(input, config, stop=stop, **kwargs)
           ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 402, in invoke
    self.generate_prompt(
    ~~~~~~~~~~~~~~~~~~~~^
        [self._convert_input(input)],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<6 lines>...
        **kwargs,
        ^^^^^^^^^
    ).generations[0][0],
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1121, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 931, in generate
    self._generate_with_cache(
    ~~~~~~~~~~~~~~~~~~~~~~~~~^
        m,
        ^^
    ...<2 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1233, in _generate_with_cache
    result = self._generate(
        messages, stop=stop, run_manager=run_manager, **kwargs
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 3051, in _generate
    _handle_client_error(e, request)
    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 145, in _handle_client_error
    raise ChatGoogleGenerativeAIError(msg) from e
langchain_google_genai.chat_models.ChatGoogleGenerativeAIError: Error calling model 'gemini-1.5-flash-001' (NOT_FOUND): 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-flash-001 is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}
\n--------------------------------\n\n--- Error at jeffe ---\nTraceback (most recent call last):
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 3047, in _generate
    response: GenerateContentResponse = self.client.models.generate_content(
                                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        **request,
        ^^^^^^^^^^
    )
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\models.py", line 5227, in generate_content
    response = self._generate_content(
        model=model, contents=contents, config=parsed_config
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\models.py", line 4009, in _generate_content
    response = self._api_client.request(
        'post', path, request_dict, http_options
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1386, in request
    response = self._request(http_request, http_options, stream=False)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1220, in _request
    return retry(self._request_once, http_request, stream)  # type: ignore[no-any-return]
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 378, in iter
    result = action(retry_state)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 400, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ~~~~~~~~~~~~~~~~~^^
  File "C:\Users\jeffe\AppData\Local\Programs\Python\Python313\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ~~~~~~~~~~~~~~~~~^^
  File "C:\Users\jeffe\AppData\Local\Programs\Python\Python313\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1199, in _request_once
    errors.APIError.raise_for_response(response)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\errors.py", line 121, in raise_for_response
    cls.raise_error(response.status_code, response_json, response)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\errors.py", line 146, in raise_error
    raise ClientError(status_code, response_json, response)
google.genai.errors.ClientError: 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-flash-001 is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\jeffe\Documents\Sites\LIA\chat\services.py", line 67, in chat
    response = self.llm.invoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=message_text)
    ])
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 2535, in invoke
    return super().invoke(input, config, stop=stop, **kwargs)
           ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 402, in invoke
    self.generate_prompt(
    ~~~~~~~~~~~~~~~~~~~~^
        [self._convert_input(input)],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<6 lines>...
        **kwargs,
        ^^^^^^^^^
    ).generations[0][0],
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1121, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 931, in generate
    self._generate_with_cache(
    ~~~~~~~~~~~~~~~~~~~~~~~~~^
        m,
        ^^
    ...<2 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1233, in _generate_with_cache
    result = self._generate(
        messages, stop=stop, run_manager=run_manager, **kwargs
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 3051, in _generate
    _handle_client_error(e, request)
    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 145, in _handle_client_error
    raise ChatGoogleGenerativeAIError(msg) from e
langchain_google_genai.chat_models.ChatGoogleGenerativeAIError: Error calling model 'gemini-1.5-flash-001' (NOT_FOUND): 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-flash-001 is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}
\n--------------------------------\n\n--- Error at jeffe ---\nTraceback (most recent call last):
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 3047, in _generate
    response: GenerateContentResponse = self.client.models.generate_content(
                                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        **request,
        ^^^^^^^^^^
    )
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\models.py", line 5227, in generate_content
    response = self._generate_content(
        model=model, contents=contents, config=parsed_config
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\models.py", line 4009, in _generate_content
    response = self._api_client.request(
        'post', path, request_dict, http_options
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1386, in request
    response = self._request(http_request, http_options, stream=False)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1220, in _request
    return retry(self._request_once, http_request, stream)  # type: ignore[no-any-return]
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 378, in iter
    result = action(retry_state)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 400, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ~~~~~~~~~~~~~~~~~^^
  File "C:\Users\jeffe\AppData\Local\Programs\Python\Python313\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ~~~~~~~~~~~~~~~~~^^
  File "C:\Users\jeffe\AppData\Local\Programs\Python\Python313\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1199, in _request_once
    errors.APIError.raise_for_response(response)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\errors.py", line 121, in raise_for_response
    cls.raise_error(response.status_code, response_json, response)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\errors.py", line 146, in raise_error
    raise ClientError(status_code, response_json, response)
google.genai.errors.ClientError: 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\jeffe\Documents\Sites\LIA\chat\services.py", line 67, in chat
    response = self.llm.invoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=message_text)
    ])
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 2535, in invoke
    return super().invoke(input, config, stop=stop, **kwargs)
           ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 402, in invoke
    self.generate_prompt(
    ~~~~~~~~~~~~~~~~~~~~^
        [self._convert_input(input)],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<6 lines>...
        **kwargs,
        ^^^^^^^^^
    ).generations[0][0],
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1121, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 931, in generate
    self._generate_with_cache(
    ~~~~~~~~~~~~~~~~~~~~~~~~~^
        m,
        ^^
    ...<2 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1233, in _generate_with_cache
    result = self._generate(
        messages, stop=stop, run_manager=run_manager, **kwargs
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 3051, in _generate
    _handle_client_error(e, request)
    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 145, in _handle_client_error
    raise ChatGoogleGenerativeAIError(msg) from e
langchain_google_genai.chat_models.ChatGoogleGenerativeAIError: Error calling model 'gemini-1.5-flash' (NOT_FOUND): 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}
\n--------------------------------\n\n--- Error at jeffe ---\nTraceback (most recent call last):
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 3047, in _generate
    response: GenerateContentResponse = self.client.models.generate_content(
                                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        **request,
        ^^^^^^^^^^
    )
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\models.py", line 5227, in generate_content
    response = self._generate_content(
        model=model, contents=contents, config=parsed_config
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\models.py", line 4009, in _generate_content
    response = self._api_client.request(
        'post', path, request_dict, http_options
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1386, in request
    response = self._request(http_request, http_options, stream=False)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1220, in _request
    return retry(self._request_once, http_request, stream)  # type: ignore[no-any-return]
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 378, in iter
    result = action(retry_state)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 420, in exc_check
    raise retry_exc.reraise()
          ~~~~~~~~~~~~~~~~~^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 187, in reraise
    raise self.last_attempt.result()
          ~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\jeffe\AppData\Local\Programs\Python\Python313\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ~~~~~~~~~~~~~~~~~^^
  File "C:\Users\jeffe\AppData\Local\Programs\Python\Python313\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1199, in _request_once
    errors.APIError.raise_for_response(response)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\errors.py", line 121, in raise_for_response
    cls.raise_error(response.status_code, response_json, response)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\errors.py", line 146, in raise_error
    raise ClientError(status_code, response_json, response)
google.genai.errors.ClientError: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\nPlease retry in 44.709554615s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '44s'}]}}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\jeffe\Documents\Sites\LIA\chat\services.py", line 67, in chat
    response = self.llm.invoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=message_text)
    ])
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 2535, in invoke
    return super().invoke(input, config, stop=stop, **kwargs)
           ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 402, in invoke
    self.generate_prompt(
    ~~~~~~~~~~~~~~~~~~~~^
        [self._convert_input(input)],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<6 lines>...
        **kwargs,
        ^^^^^^^^^
    ).generations[0][0],
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1121, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 931, in generate
    self._generate_with_cache(
    ~~~~~~~~~~~~~~~~~~~~~~~~~^
        m,
        ^^
    ...<2 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1233, in _generate_with_cache
    result = self._generate(
        messages, stop=stop, run_manager=run_manager, **kwargs
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 3051, in _generate
    _handle_client_error(e, request)
    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 145, in _handle_client_error
    raise ChatGoogleGenerativeAIError(msg) from e
langchain_google_genai.chat_models.ChatGoogleGenerativeAIError: Error calling model 'gemini-2.0-flash' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\nPlease retry in 44.709554615s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '44s'}]}}
\n--------------------------------\n\n--- Error at jeffe ---\nTraceback (most recent call last):
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 3047, in _generate
    response: GenerateContentResponse = self.client.models.generate_content(
                                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        **request,
        ^^^^^^^^^^
    )
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\models.py", line 5227, in generate_content
    response = self._generate_content(
        model=model, contents=contents, config=parsed_config
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\models.py", line 4009, in _generate_content
    response = self._api_client.request(
        'post', path, request_dict, http_options
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1386, in request
    response = self._request(http_request, http_options, stream=False)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1220, in _request
    return retry(self._request_once, http_request, stream)  # type: ignore[no-any-return]
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 378, in iter
    result = action(retry_state)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 420, in exc_check
    raise retry_exc.reraise()
          ~~~~~~~~~~~~~~~~~^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 187, in reraise
    raise self.last_attempt.result()
          ~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\jeffe\AppData\Local\Programs\Python\Python313\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ~~~~~~~~~~~~~~~~~^^
  File "C:\Users\jeffe\AppData\Local\Programs\Python\Python313\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1199, in _request_once
    errors.APIError.raise_for_response(response)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\errors.py", line 121, in raise_for_response
    cls.raise_error(response.status_code, response_json, response)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\errors.py", line 146, in raise_error
    raise ClientError(status_code, response_json, response)
google.genai.errors.ClientError: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-lite\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-lite\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash-lite\nPlease retry in 59.17781075s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash-lite'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash-lite'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash-lite'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '59s'}]}}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\jeffe\Documents\Sites\LIA\chat\services.py", line 67, in chat
    response = self.llm.invoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=message_text)
    ])
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 2535, in invoke
    return super().invoke(input, config, stop=stop, **kwargs)
           ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 402, in invoke
    self.generate_prompt(
    ~~~~~~~~~~~~~~~~~~~~^
        [self._convert_input(input)],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<6 lines>...
        **kwargs,
        ^^^^^^^^^
    ).generations[0][0],
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1121, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 931, in generate
    self._generate_with_cache(
    ~~~~~~~~~~~~~~~~~~~~~~~~~^
        m,
        ^^
    ...<2 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1233, in _generate_with_cache
    result = self._generate(
        messages, stop=stop, run_manager=run_manager, **kwargs
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 3051, in _generate
    _handle_client_error(e, request)
    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 145, in _handle_client_error
    raise ChatGoogleGenerativeAIError(msg) from e
langchain_google_genai.chat_models.ChatGoogleGenerativeAIError: Error calling model 'gemini-2.0-flash-lite' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-lite\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-lite\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash-lite\nPlease retry in 59.17781075s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash-lite'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash-lite'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash-lite'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '59s'}]}}
\n--------------------------------\n\n--- Error at jeffe ---\nTraceback (most recent call last):
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 3047, in _generate
    response: GenerateContentResponse = self.client.models.generate_content(
                                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        **request,
        ^^^^^^^^^^
    )
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\models.py", line 5227, in generate_content
    response = self._generate_content(
        model=model, contents=contents, config=parsed_config
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\models.py", line 4009, in _generate_content
    response = self._api_client.request(
        'post', path, request_dict, http_options
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1386, in request
    response = self._request(http_request, http_options, stream=False)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1220, in _request
    return retry(self._request_once, http_request, stream)  # type: ignore[no-any-return]
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 378, in iter
    result = action(retry_state)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 420, in exc_check
    raise retry_exc.reraise()
          ~~~~~~~~~~~~~~~~~^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 187, in reraise
    raise self.last_attempt.result()
          ~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\jeffe\AppData\Local\Programs\Python\Python313\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ~~~~~~~~~~~~~~~~~^^
  File "C:\Users\jeffe\AppData\Local\Programs\Python\Python313\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1199, in _request_once
    errors.APIError.raise_for_response(response)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\errors.py", line 121, in raise_for_response
    cls.raise_error(response.status_code, response_json, response)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\errors.py", line 146, in raise_error
    raise ClientError(status_code, response_json, response)
google.genai.errors.ClientError: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash-lite\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-lite\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-lite\nPlease retry in 4.578332592s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash-lite'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.0-flash-lite', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.0-flash-lite', 'location': 'global'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '4s'}]}}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\jeffe\Documents\Sites\LIA\chat\services.py", line 67, in chat
    response = self.llm.invoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=message_text)
    ])
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 2535, in invoke
    return super().invoke(input, config, stop=stop, **kwargs)
           ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 402, in invoke
    self.generate_prompt(
    ~~~~~~~~~~~~~~~~~~~~^
        [self._convert_input(input)],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<6 lines>...
        **kwargs,
        ^^^^^^^^^
    ).generations[0][0],
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1121, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 931, in generate
    self._generate_with_cache(
    ~~~~~~~~~~~~~~~~~~~~~~~~~^
        m,
        ^^
    ...<2 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1233, in _generate_with_cache
    result = self._generate(
        messages, stop=stop, run_manager=run_manager, **kwargs
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 3051, in _generate
    _handle_client_error(e, request)
    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 145, in _handle_client_error
    raise ChatGoogleGenerativeAIError(msg) from e
langchain_google_genai.chat_models.ChatGoogleGenerativeAIError: Error calling model 'gemini-2.0-flash-lite' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash-lite\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-lite\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-lite\nPlease retry in 4.578332592s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash-lite'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.0-flash-lite', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.0-flash-lite', 'location': 'global'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '4s'}]}}
\n--------------------------------\n\n--- Error at jeffe ---\nTraceback (most recent call last):
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 3047, in _generate
    response: GenerateContentResponse = self.client.models.generate_content(
                                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        **request,
        ^^^^^^^^^^
    )
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\models.py", line 5227, in generate_content
    response = self._generate_content(
        model=model, contents=contents, config=parsed_config
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\models.py", line 4009, in _generate_content
    response = self._api_client.request(
        'post', path, request_dict, http_options
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1386, in request
    response = self._request(http_request, http_options, stream=False)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1220, in _request
    return retry(self._request_once, http_request, stream)  # type: ignore[no-any-return]
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 378, in iter
    result = action(retry_state)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 420, in exc_check
    raise retry_exc.reraise()
          ~~~~~~~~~~~~~~~~~^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 187, in reraise
    raise self.last_attempt.result()
          ~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\jeffe\AppData\Local\Programs\Python\Python313\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ~~~~~~~~~~~~~~~~~^^
  File "C:\Users\jeffe\AppData\Local\Programs\Python\Python313\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1199, in _request_once
    errors.APIError.raise_for_response(response)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\errors.py", line 121, in raise_for_response
    cls.raise_error(response.status_code, response_json, response)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\errors.py", line 146, in raise_error
    raise ClientError(status_code, response_json, response)
google.genai.errors.ClientError: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash-lite\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-lite\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-lite\nPlease retry in 8.159262303s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash-lite'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash-lite'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.0-flash-lite', 'location': 'global'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '8s'}]}}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\jeffe\Documents\Sites\LIA\chat\services.py", line 67, in chat
    response = self.llm.invoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=message_text)
    ])
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 2535, in invoke
    return super().invoke(input, config, stop=stop, **kwargs)
           ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 402, in invoke
    self.generate_prompt(
    ~~~~~~~~~~~~~~~~~~~~^
        [self._convert_input(input)],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<6 lines>...
        **kwargs,
        ^^^^^^^^^
    ).generations[0][0],
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1121, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 931, in generate
    self._generate_with_cache(
    ~~~~~~~~~~~~~~~~~~~~~~~~~^
        m,
        ^^
    ...<2 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1233, in _generate_with_cache
    result = self._generate(
        messages, stop=stop, run_manager=run_manager, **kwargs
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 3051, in _generate
    _handle_client_error(e, request)
    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 145, in _handle_client_error
    raise ChatGoogleGenerativeAIError(msg) from e
langchain_google_genai.chat_models.ChatGoogleGenerativeAIError: Error calling model 'gemini-2.0-flash-lite' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash-lite\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-lite\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-lite\nPlease retry in 8.159262303s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash-lite'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash-lite'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.0-flash-lite', 'location': 'global'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '8s'}]}}
\n--------------------------------\n\n--- Error at jeffe ---\nTraceback (most recent call last):
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 3047, in _generate
    response: GenerateContentResponse = self.client.models.generate_content(
                                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        **request,
        ^^^^^^^^^^
    )
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\models.py", line 5227, in generate_content
    response = self._generate_content(
        model=model, contents=contents, config=parsed_config
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\models.py", line 4009, in _generate_content
    response = self._api_client.request(
        'post', path, request_dict, http_options
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1386, in request
    response = self._request(http_request, http_options, stream=False)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1220, in _request
    return retry(self._request_once, http_request, stream)  # type: ignore[no-any-return]
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 378, in iter
    result = action(retry_state)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 420, in exc_check
    raise retry_exc.reraise()
          ~~~~~~~~~~~~~~~~~^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 187, in reraise
    raise self.last_attempt.result()
          ~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\jeffe\AppData\Local\Programs\Python\Python313\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ~~~~~~~~~~~~~~~~~^^
  File "C:\Users\jeffe\AppData\Local\Programs\Python\Python313\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1199, in _request_once
    errors.APIError.raise_for_response(response)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\errors.py", line 121, in raise_for_response
    cls.raise_error(response.status_code, response_json, response)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\errors.py", line 146, in raise_error
    raise ClientError(status_code, response_json, response)
google.genai.errors.ClientError: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\nPlease retry in 57.121243954s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '57s'}]}}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\jeffe\Documents\Sites\LIA\chat\services.py", line 86, in chat
    response = self.llm.invoke(messages_payload)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 2535, in invoke
    return super().invoke(input, config, stop=stop, **kwargs)
           ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 402, in invoke
    self.generate_prompt(
    ~~~~~~~~~~~~~~~~~~~~^
        [self._convert_input(input)],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<6 lines>...
        **kwargs,
        ^^^^^^^^^
    ).generations[0][0],
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1121, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 931, in generate
    self._generate_with_cache(
    ~~~~~~~~~~~~~~~~~~~~~~~~~^
        m,
        ^^
    ...<2 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1233, in _generate_with_cache
    result = self._generate(
        messages, stop=stop, run_manager=run_manager, **kwargs
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 3051, in _generate
    _handle_client_error(e, request)
    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 145, in _handle_client_error
    raise ChatGoogleGenerativeAIError(msg) from e
langchain_google_genai.chat_models.ChatGoogleGenerativeAIError: Error calling model 'gemini-flash-latest' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\nPlease retry in 57.121243954s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '57s'}]}}
\n--------------------------------\n\n--- Error at jeffe ---\nTraceback (most recent call last):
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 3047, in _generate
    response: GenerateContentResponse = self.client.models.generate_content(
                                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        **request,
        ^^^^^^^^^^
    )
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\models.py", line 5227, in generate_content
    response = self._generate_content(
        model=model, contents=contents, config=parsed_config
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\models.py", line 4009, in _generate_content
    response = self._api_client.request(
        'post', path, request_dict, http_options
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1386, in request
    response = self._request(http_request, http_options, stream=False)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1220, in _request
    return retry(self._request_once, http_request, stream)  # type: ignore[no-any-return]
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 378, in iter
    result = action(retry_state)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 400, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ~~~~~~~~~~~~~~~~~^^
  File "C:\Users\jeffe\AppData\Local\Programs\Python\Python313\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ~~~~~~~~~~~~~~~~~^^
  File "C:\Users\jeffe\AppData\Local\Programs\Python\Python313\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1199, in _request_once
    errors.APIError.raise_for_response(response)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\errors.py", line 121, in raise_for_response
    cls.raise_error(response.status_code, response_json, response)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\errors.py", line 146, in raise_error
    raise ClientError(status_code, response_json, response)
google.genai.errors.ClientError: 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\jeffe\Documents\Sites\LIA\chat\services.py", line 86, in chat
    response = self.llm.invoke(messages_payload)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 2535, in invoke
    return super().invoke(input, config, stop=stop, **kwargs)
           ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 402, in invoke
    self.generate_prompt(
    ~~~~~~~~~~~~~~~~~~~~^
        [self._convert_input(input)],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<6 lines>...
        **kwargs,
        ^^^^^^^^^
    ).generations[0][0],
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1121, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 931, in generate
    self._generate_with_cache(
    ~~~~~~~~~~~~~~~~~~~~~~~~~^
        m,
        ^^
    ...<2 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1233, in _generate_with_cache
    result = self._generate(
        messages, stop=stop, run_manager=run_manager, **kwargs
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 3051, in _generate
    _handle_client_error(e, request)
    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 145, in _handle_client_error
    raise ChatGoogleGenerativeAIError(msg) from e
langchain_google_genai.chat_models.ChatGoogleGenerativeAIError: Error calling model 'gemini-1.5-flash' (NOT_FOUND): 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}
\n--------------------------------\n\n--- Error at jeffe ---\nTraceback (most recent call last):
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 3047, in _generate
    response: GenerateContentResponse = self.client.models.generate_content(
                                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        **request,
        ^^^^^^^^^^
    )
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\models.py", line 5227, in generate_content
    response = self._generate_content(
        model=model, contents=contents, config=parsed_config
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\models.py", line 4009, in _generate_content
    response = self._api_client.request(
        'post', path, request_dict, http_options
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1386, in request
    response = self._request(http_request, http_options, stream=False)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1220, in _request
    return retry(self._request_once, http_request, stream)  # type: ignore[no-any-return]
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 378, in iter
    result = action(retry_state)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 420, in exc_check
    raise retry_exc.reraise()
          ~~~~~~~~~~~~~~~~~^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 187, in reraise
    raise self.last_attempt.result()
          ~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\jeffe\AppData\Local\Programs\Python\Python313\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ~~~~~~~~~~~~~~~~~^^
  File "C:\Users\jeffe\AppData\Local\Programs\Python\Python313\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1199, in _request_once
    errors.APIError.raise_for_response(response)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\errors.py", line 121, in raise_for_response
    cls.raise_error(response.status_code, response_json, response)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\errors.py", line 146, in raise_error
    raise ClientError(status_code, response_json, response)
google.genai.errors.ClientError: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\nPlease retry in 34.423637569s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.0-flash', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '34s'}]}}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\jeffe\Documents\Sites\LIA\chat\services.py", line 86, in chat
    response = self.llm.invoke(messages_payload)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 2535, in invoke
    return super().invoke(input, config, stop=stop, **kwargs)
           ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 402, in invoke
    self.generate_prompt(
    ~~~~~~~~~~~~~~~~~~~~^
        [self._convert_input(input)],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<6 lines>...
        **kwargs,
        ^^^^^^^^^
    ).generations[0][0],
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1121, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 931, in generate
    self._generate_with_cache(
    ~~~~~~~~~~~~~~~~~~~~~~~~~^
        m,
        ^^
    ...<2 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1233, in _generate_with_cache
    result = self._generate(
        messages, stop=stop, run_manager=run_manager, **kwargs
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 3051, in _generate
    _handle_client_error(e, request)
    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 145, in _handle_client_error
    raise ChatGoogleGenerativeAIError(msg) from e
langchain_google_genai.chat_models.ChatGoogleGenerativeAIError: Error calling model 'gemini-2.0-flash' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\nPlease retry in 34.423637569s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.0-flash', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '34s'}]}}
\n--------------------------------\n\n--- Error at jeffe ---\nTraceback (most recent call last):
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 3047, in _generate
    response: GenerateContentResponse = self.client.models.generate_content(
                                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        **request,
        ^^^^^^^^^^
    )
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\models.py", line 5227, in generate_content
    response = self._generate_content(
        model=model, contents=contents, config=parsed_config
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\models.py", line 4009, in _generate_content
    response = self._api_client.request(
        'post', path, request_dict, http_options
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1386, in request
    response = self._request(http_request, http_options, stream=False)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1220, in _request
    return retry(self._request_once, http_request, stream)  # type: ignore[no-any-return]
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 378, in iter
    result = action(retry_state)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 420, in exc_check
    raise retry_exc.reraise()
          ~~~~~~~~~~~~~~~~~^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 187, in reraise
    raise self.last_attempt.result()
          ~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\jeffe\AppData\Local\Programs\Python\Python313\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ~~~~~~~~~~~~~~~~~^^
  File "C:\Users\jeffe\AppData\Local\Programs\Python\Python313\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1199, in _request_once
    errors.APIError.raise_for_response(response)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\errors.py", line 121, in raise_for_response
    cls.raise_error(response.status_code, response_json, response)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\errors.py", line 146, in raise_error
    raise ClientError(status_code, response_json, response)
google.genai.errors.ClientError: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.5-pro\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.5-pro\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.5-pro\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.5-pro\nPlease retry in 51.489864404s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerDay-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'model': 'gemini-2.5-pro', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-pro'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '51s'}]}}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\jeffe\Documents\Sites\LIA\chat\services.py", line 86, in chat
    response = self.llm.invoke(messages_payload)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 2535, in invoke
    return super().invoke(input, config, stop=stop, **kwargs)
           ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 402, in invoke
    self.generate_prompt(
    ~~~~~~~~~~~~~~~~~~~~^
        [self._convert_input(input)],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<6 lines>...
        **kwargs,
        ^^^^^^^^^
    ).generations[0][0],
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1121, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 931, in generate
    self._generate_with_cache(
    ~~~~~~~~~~~~~~~~~~~~~~~~~^
        m,
        ^^
    ...<2 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1233, in _generate_with_cache
    result = self._generate(
        messages, stop=stop, run_manager=run_manager, **kwargs
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 3051, in _generate
    _handle_client_error(e, request)
    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 145, in _handle_client_error
    raise ChatGoogleGenerativeAIError(msg) from e
langchain_google_genai.chat_models.ChatGoogleGenerativeAIError: Error calling model 'gemini-pro-latest' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.5-pro\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.5-pro\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.5-pro\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.5-pro\nPlease retry in 51.489864404s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerDay-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'model': 'gemini-2.5-pro', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-pro'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '51s'}]}}
\n--------------------------------\n\n--- Error at jeffe ---\nTraceback (most recent call last):
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 3047, in _generate
    response: GenerateContentResponse = self.client.models.generate_content(
                                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        **request,
        ^^^^^^^^^^
    )
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\models.py", line 5227, in generate_content
    response = self._generate_content(
        model=model, contents=contents, config=parsed_config
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\models.py", line 4009, in _generate_content
    response = self._api_client.request(
        'post', path, request_dict, http_options
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1386, in request
    response = self._request(http_request, http_options, stream=False)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1220, in _request
    return retry(self._request_once, http_request, stream)  # type: ignore[no-any-return]
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 378, in iter
    result = action(retry_state)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 400, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ~~~~~~~~~~~~~~~~~^^
  File "C:\Users\jeffe\AppData\Local\Programs\Python\Python313\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ~~~~~~~~~~~~~~~~~^^
  File "C:\Users\jeffe\AppData\Local\Programs\Python\Python313\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\tenacity\__init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\_api_client.py", line 1199, in _request_once
    errors.APIError.raise_for_response(response)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\errors.py", line 121, in raise_for_response
    cls.raise_error(response.status_code, response_json, response)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\google\genai\errors.py", line 146, in raise_error
    raise ClientError(status_code, response_json, response)
google.genai.errors.ClientError: 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'Developer instruction is not enabled for models/gemma-3-1b-it', 'status': 'INVALID_ARGUMENT'}}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\jeffe\Documents\Sites\LIA\chat\services.py", line 86, in chat
    response = self.llm.invoke(messages_payload)
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 2535, in invoke
    return super().invoke(input, config, stop=stop, **kwargs)
           ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 402, in invoke
    self.generate_prompt(
    ~~~~~~~~~~~~~~~~~~~~^
        [self._convert_input(input)],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<6 lines>...
        **kwargs,
        ^^^^^^^^^
    ).generations[0][0],
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1121, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 931, in generate
    self._generate_with_cache(
    ~~~~~~~~~~~~~~~~~~~~~~~~~^
        m,
        ^^
    ...<2 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1233, in _generate_with_cache
    result = self._generate(
        messages, stop=stop, run_manager=run_manager, **kwargs
    )
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 3051, in _generate
    _handle_client_error(e, request)
    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\jeffe\Documents\Sites\LIA\venv\Lib\site-packages\langchain_google_genai\chat_models.py", line 145, in _handle_client_error
    raise ChatGoogleGenerativeAIError(msg) from e
langchain_google_genai.chat_models.ChatGoogleGenerativeAIError: Error calling model 'gemma-3-1b-it' (INVALID_ARGUMENT): 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'Developer instruction is not enabled for models/gemma-3-1b-it', 'status': 'INVALID_ARGUMENT'}}
\n--------------------------------\n